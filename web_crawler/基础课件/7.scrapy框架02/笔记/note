- 剩余内容：
    - scrapy的五大核心组件介绍
    - 请求传参实现的深度爬取
    - 中间件机制
    - 大文件下载
    - CrawlSpider
    - 分布式
    - 增量式


五大核心组件
    - 目的：
        - 1.大概了解scrapy的运行机制
        - 2.分布式铺垫
    - 组件的作用：
        引擎(Scrapy)
            用来处理整个系统的数据流处理, 触发事务(框架核心)
        调度器(Scheduler)
            用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址
        下载器(Downloader)
            用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)
        爬虫(Spiders)
            爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面
        项目管道(Pipeline)
            负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。

请求传参实现的深度爬取
- 深度爬取：爬取的数据没有在同一张页面中（首页数据+详情页数据）
- 在scrapy中如果没有请求传参我们是无法持久化存储数据
- 实现方式：
    - scrapy.Request(url,callback,meta)
        - meta是一个字典，可以将meta传递给callback
    - callback取出meta：
        - response.meta


中间件：
- 作用：批量拦截请求和响应
- 爬虫中间件
- 下载中间件（推荐）
    - 拦截请求：
        - 篡改请求url
        - 伪装请求头信息
            - UA
            - Cookie
        - 设置请求代理（重点）
    - 拦截响应
        - 篡改响应数据

    - 代理操作必须使用中间件才可以实现
        - process_exception：
            - request.meta['proxy'] = 'http://ip:port'



大文件下载：大文件数据是在管道中请求到的
    - 下属管道类是scrapy封装好的我们直接用即可：
    - from scrapy.pipelines.images import ImagesPipeline #提供了数据下载功能
    - 重写该管道类的三个方法：
        - get_media_requests
            - 对图片地址发起请求
        - file_path
            - 返回图片名称即可
        - item_completed
            - 返回item，将其返回给下一个即将被执行的管道类
        - 在配置文件中添加：
            - IMAGES_STORE = 'dirName'


- settings.py中的常用配置
    增加并发：
        默认scrapy开启的并发线程为32个，可以适当进行增加。在settings配置文件中修改CONCURRENT_REQUESTS = 100值为100,并发设置成了为100。

    降低日志级别：
        在运行scrapy时，会有大量日志信息的输出，为了减少CPU的使用率。可以设置log输出信息为INFO或者ERROR即可。在配置文件中编写：LOG_LEVEL = ‘INFO’

    禁止cookie：
        如果不是真的需要cookie，则在scrapy爬取数据时可以禁止cookie从而减少CPU的使用率，提升爬取效率。在配置文件中编写：COOKIES_ENABLED = False

    禁止重试：
        对失败的HTTP进行重新请求（重试）会减慢爬取速度，因此可以禁止重试。在配置文件中编写：RETRY_ENABLED = False

    减少下载超时：
        如果对一个非常慢的链接进行爬取，减少下载超时可以能让卡住的链接快速被放弃，从而提升效率。在配置文件中进行编写：DOWNLOAD_TIMEOUT = 10 超时时间为10s


- CrawlSpider
    - 其实SPider的一个子类。Spider爬虫文件中爬虫类的父类。
        - 子类的功能一定是多余父类
    - 作用：被用作于专业实现全站数据爬取
        - 将一个页面下所有页码对应的数据进行爬取
    - 基本使用：
        - 1.创建一个工程
        - 2.cd 工程
        - 3.创建一个基于CrawlSpider的爬虫文件
            - scrapy genspider -t crawl SpiderName www.xxx.com
        - 4.执行工程
    - 注意：
        - 1.一个链接提取器对应一个规则解析器（多个链接提取器和多个规则解析器）
        - 2.在实现深度爬取的过程中需要和scrapy.Request()结合使用
            - 明日讲
    - 面试题：
        - 如何将一个网站中全站所有的链接都进行爬取。

    任务：尝试使用CrawlSpider实现深度爬取。

- 分布式
- 增量式
